# ğŸ§  Adobe Hackathon Round 1B â€” Persona-Aware PDF Section Ranker

A solution to extract and rank the most relevant PDF sections for a given **persona** and **job-to-be-done**, using optimized ONNX-based embeddings and contextual keyword matching.

---

## ğŸ“Œ Problem Statement

> Given multiple PDFs, a **persona** (e.g., travel agent, researcher) and their **task**, extract the top 5 most relevant sections across documents â€” and return them in a structured JSON format.

---

## Team
Name : InnovationNation 

Members : 
- [Shashank Tiwari](https://github.com/Shashank110205)
- [Badal Singh](https://github.com/Badalsingh2)
- [Vivian Ludrick](https://github.com/vivalchemy)

---

## ğŸš€ Features

- ğŸ”¹ **E5-Small-V2 ONNX** model for efficient, offline-compatible embeddings
- ğŸ”¹ **Dynamic keyword expansion** using a universal vocabulary
- ğŸ”¹ **Robust heading-based chunking** using font and spacing logic
- ğŸ”¹ **Caching & batching** for speed and performance
- ğŸ”¹ **Contextual refinement** of extracted text and section titles

---

## ğŸ§  Approach Summary

<details>
<summary>ğŸ” Click to expand</summary>

### ğŸ”¹ Step 1: Input Parsing
- Input JSON defines:
  - `persona`
  - `job_to_be_done`
  - List of PDFs to process

### ğŸ”¹ Step 2: Chunk Extraction
- PDFs processed using **PyMuPDF**
- Sections extracted based on:
  - Font size
  - Boldness
  - Heading heuristics

### ğŸ”¹ Step 3: Embedding Generation
- Each query and section is converted into embeddings using ONNX
- `[query:]` and `[passage:]` prefixes are used (as per E5 paper)
- Results are normalized and cached

### ğŸ”¹ Step 4: Scoring & Ranking
- Each chunk scored by:
  - **Cosine similarity** with query
  - **Keyword match score**
- Top 5 chunks selected across documents (1 per document)

### ğŸ”¹ Step 5: Output
- Final JSON includes:
  - Metadata (persona, task, timestamp)
  - Extracted sections
  - Refined content for each section

</details>

---

## ğŸ—‚ï¸ Folder Structure

```.
â”œâ”€â”€ main.py # Optimized pipeline using ONNX
â”œâ”€â”€ main1.py # NLTK-based simpler baseline version
â”œâ”€â”€ input.json # Input with persona, task, and documents
â”œâ”€â”€ challenge1b_output.json # Final structured output
â”œâ”€â”€ optimized_util/ # Optimized embedding, PDF, scoring utilities
â”‚ â”œâ”€â”€ embedding_generator.py
â”‚ â”œâ”€â”€ output_builder.py
â”‚ â”œâ”€â”€ pdf_processor.py
â”‚ â””â”€â”€ text_utils.py
â”œâ”€â”€ utils/ # Simpler utilities for main1.py
â”œâ”€â”€ tokenizer/ # Tokenizer JSON for ONNX model
â”œâ”€â”€ pdf/ # Folder containing input PDF files
â””â”€â”€ README.md # This file
```

---

## ğŸ“¥ Installation Steps:

1. Clone the git repository
    ```bash
    git clone git@github.com:Shashank110205/Adobe_Round1B.git
    ```

2. Move into the repository folder
    ```bash
    cd Adobe_Round1B
    ```

3. Build the docker image(requires internet connection)
    ```bash
    docker compose build
    ```

    or build the image manually

    ```bash
    docker build -t pdf-processor .
    ```

4. Run the docker image(works offline)
    ```bash
    docker run -it --rm \
      --name pdf-processor \
      -v "<path_to_the_input_directory>:/app/input" \
      pdf-processor
    ```

>[!NOTE]
> The structure of the input directory should be as follows:
>```
> .
> â”œâ”€â”€ challenge1b_input.json
> â”œâ”€â”€ challenge1b_output.json
> â””â”€â”€ PDFs
>     â”œâ”€â”€ pdf1.pdf
>     â”œâ”€â”€ pdf2.pdf
>     â”œâ”€â”€ pdf3.pdf
>     â”œâ”€â”€ ...
>     â”œâ”€â”€ pdf16.pdf
> ```
