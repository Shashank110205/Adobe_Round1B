# ğŸ“˜ Approach Explanation: Persona-Aware Document Intelligence System

## ğŸ§  Overview

This system is designed to intelligently extract the most relevant sections from a set of PDF documents based on a userâ€™s *persona* and a *job to be done*. It leverages ONNX-based sentence embeddings, contextual keyword extraction, semantic similarity, and PDF parsing to return ranked, refined, and meaningful document segments with rich metadata.

---

## ğŸ”§ Key Components

### 1. **Embedding Generator**
- File: `embedding_generator.py`
- Purpose: Generates semantic vector embeddings using a pre-trained ONNX version of the `e5-small-v2` model.
- Features:
  - Uses `onnxruntime` for fast inference.
  - Tokenization via `tokenizers` library.
  - Mean pooling + cosine normalization for embedding representation.
  - Caching support for performance.
  - Supports both single and batch embedding generation.

---

### 2. **PDF Chunk Extractor**
- File: `pdf_processor.py`
- Purpose: Converts PDF pages into structured text chunks using **PyMuPDF** (`fitz`).
- Strategy:
  - Detects headings and sections based on font size, boldness, center alignment, and vertical spacing.
  - Aggregates content between detected headings into logical "chunks".
  - Returns cleaned, page-wise sections with detected titles and body text.

---

### 3. **Text Utilities**
- File: `text_utils.py`
- Includes key utilities:
  
  #### a. `refine_text(text)`
  - Extracts readable, natural language sentences from raw chunk text.
  - Uses punctuation and quote-awareness to prevent sentence fragmentation.

  #### b. `extract_keywords(text)`
  - Identifies top-n frequent, meaningful words from the text.
  
  #### c. `generate_dynamic_keywords(persona, job, embedder)`
  - Uses semantic similarity to expand base keywords using a **universal vocabulary**.
  - Embeds both base and vocabulary words, filters based on cosine similarity.

  #### d. `generate_contextual_title(text, query_keywords, original_title, embedder)`
  - Automatically generates section titles when original ones are missing or uninformative.
  - Uses semantic matching of phrases to the query intent.

  #### e. `calculate_cosine_similarity(vec1, vec2)`
  - Robust cosine similarity computation with zero-div checks.

---

### 4. **Output Builder**
- File: `output_builder.py`
- Structures the final output into a JSON format with:
  - Metadata (persona, job, timestamps)
  - Extracted section titles
  - Refined section content for downstream applications
- Output: `challenge1b_output.json`

---

## âš™ï¸ Main Pipeline: `main()`

### Step-by-Step Flow:

#### âœ… 1. **Input Parsing**
- Reads `input.json`:
  ```json
  {
    "persona": {"role": "Travel Planner"},
    "job_to_be_done": {"task": "Plan a 5-day tour in Italy"},
    "documents": [{"filename": "italy_travel.pdf"}, ...]
  }
  ```

#### ğŸ§  2. **Query Embedding Generation**
- Combines persona and task into a semantic query.
- Extracts and expands keywords using `generate_dynamic_keywords()`.

#### ğŸ“„ 3. **Document Chunking**
- Extracts and structures content from up to `MAX_DOCUMENTS` PDF files using `ThreadPoolExecutor`.

#### ğŸ§¹ 4. **Preprocessing**
- Cleans each chunk.
- Refines text.
- Scores keyword matches for each chunk.

#### ğŸ” 5. **Semantic Scoring**
- Embeds all relevant text chunks using `get_embeddings_batch()`.
- Calculates similarity with the query embedding.
- Combines:
  - Cosine similarity
  - Keyword match score
  - Dynamic keyword boosting

#### ğŸ† 6. **Top Chunk Selection**
- Sorts all chunks by `final_score`.
- Selects up to `MAX_TOP_CHUNKS`, ensuring **document diversity** (max 2 per file).
- Uses `generate_contextual_title()` to assign informative titles.

#### ğŸ“ 7. **Output Generation**
- Formats everything into a standardized output structure.
- Writes final result to `challenge1b_output.json`.

---

## ğŸ”‘ Techniques Used

| Technique                        | Purpose                                     |
|----------------------------------|---------------------------------------------|
| ONNX Inference with `e5-small-v2`| Fast, accurate sentence embeddings          |
| Mean Pooling + Normalization     | Stable sentence representations             |
| PDF Section Detection            | Structure-aware content segmentation        |
| Cosine Similarity                | Semantic comparison                         |
| Dynamic Keyword Expansion        | Context-aware matching                      |
| Title Regeneration               | Makes content self-explanatory              |
| Multithreading (`ThreadPoolExecutor`) | Efficient document parsing              |

---

## ğŸ“ Folder Structure (Simplified)

```
project/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ input.json
â”œâ”€â”€ challenge1b_output.json
â”‚
â”œâ”€â”€ optimized_util/
â”‚   â”œâ”€â”€ embedding_generator.py
â”‚   â”œâ”€â”€ pdf_processor.py
â”‚   â”œâ”€â”€ text_utils.py
â”‚   â””â”€â”€ output_builder.py
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ e5-small-v2-onnx/
â”‚       â”œâ”€â”€ model.onnx
â”‚       â””â”€â”€ tokenizer/tokenizer.json
â”‚
â””â”€â”€ pdf/
    â””â”€â”€ *.pdf
```

---

## âœ… Final Output Format

Sample schema from `challenge1b_output.json`:
```json
{
  "metadata": { ... },
  "extracted_sections": [
    {
      "document": "italy_travel.pdf",
      "page_number": 5,
      "section_title": "Day 1 Florence Tour",
      "importance_rank": 1
    }
  ],
  "subsection_analysis": [
    {
      "document": "italy_travel.pdf",
      "refined_text": "Begin your journey in Florence with a guided tour...",
      "page_number": 5,
      "page_number_constraints": {
        "start": 5,
        "end": 5
      }
    }
  ]
}
```

---

## ğŸ§ª Performance and Limitations

| Metric                   | Value             |
|--------------------------|-------------------|
| Avg Processing Time      | ~10â€“15 seconds (depending on file size) |
| Max PDF Size             | 10 PDFs            |
| Max Extracted Chunks     | 5 high-quality sections |
| Limitation               | Heading detection might miss in scanned/non-standard PDFs |

---

## ğŸ“Œ Suggestions for Future Improvements

- Add OCR support for scanned documents (e.g., using Tesseract).
- Visual PDF preview with highlighted segments.
- UI for persona/job inputs and result exploration.
- GPU inference for faster embedding generation.